{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b7757c-f554-4d8f-a5f4-3f7217392177",
   "metadata": {},
   "source": [
    "# 決定木"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aacddf-87ce-4f1e-b26a-5d5d26a0c1c4",
   "metadata": {},
   "source": [
    "## 【問題1】不純度を求める関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871793a-5c8d-4c7b-8859-d9d4b58d22f5",
   "metadata": {},
   "source": [
    "### ノード の ジニ不純度 を計算する関数を作成してください。ノード $t$ に対するジニ不純度 $I(t)$ は以下の数式で求まります。クラスが混じり合っているほどジニ不純度は高くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8e158-d82d-4e7a-a80c-07c43cd48557",
   "metadata": {},
   "source": [
    "$$\n",
    "I(t)=1−∑^K_{i=1} P^2(Ci|t)=1−∑^K_{i=1}(\\frac{N_{t,i}}{N_{t,all}})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3fa080-70e1-456a-95b5-92f6e655ea5e",
   "metadata": {},
   "source": [
    "まずは簡単な例を作り、手計算と関数の結果を比較してください。\n",
    "\n",
    "《例》\n",
    "\n",
    "クラス1:サンプル数15, クラス2:サンプル数15 → ジニ不純度0.500  \n",
    "クラス1:サンプル数15, クラス2:サンプル数15, クラス3:サンプル数15 → ジニ不純度0.667  \n",
    "クラス1:サンプル数18, クラス2:サンプル数12 → ジニ不純度0.480  \n",
    "クラス1:サンプル数30, クラス2:サンプル数0 → ジニ不純度0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d2f2de-4a07-466d-a0b1-53c04fccfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(X):\n",
    "    # サンプルの総数\n",
    "    sum = np.sum(X)\n",
    "    # シグマの繰り返しを保管する変数\n",
    "    sigma = 0\n",
    "    # サンプルの数だけ繰り返す\n",
    "    for i in range(X.shape[0]):\n",
    "        # ジニ不純度を計算する式\n",
    "        sigma += (X[i]/sum)**2\n",
    "    ans = 1 - sigma\n",
    "    # 求めた不純度を返す\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab3f3e-3979-458c-a657-71fa0e0b4e63",
   "metadata": {},
   "source": [
    "## 【問題2】情報利得を求める関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e442b-aacb-4049-845d-ec2365492bdd",
   "metadata": {},
   "source": [
    "### 次に、ノード間の 情報利得 を計算する関数を作成してください。問題1で作成したジニ不純度 $I(t)$ を計算する関数を呼び出して使います。情報利得$IG$は以下の数式で求まります。うまく分けられている時ほど情報利得は大きくなります。\n",
    "\n",
    "### ここで分岐は2つのみであるため、分岐先を「左側のノード・右側のノード」と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6944b9-b84a-4784-a2c6-a4ddf41e471a",
   "metadata": {},
   "source": [
    "$$\n",
    "IG(p)=I(p)−\\frac{N_{left,all}}{N_{p,all}}I(left)− \\frac{N_{right,all}}{N_{p,all}}I(right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dffac9-0963-4419-ac26-525821ddfb1d",
   "metadata": {},
   "source": [
    "まずは簡単な例を作り、手計算と関数の結果を比較してください。\n",
    "\n",
    "《例》\n",
    "\n",
    "左ノードクラス1:サンプル数10  \n",
    "左ノードクラス2:サンプル数30  \n",
    "右ノードクラス1:サンプル数20  \n",
    "右ノードクラス2:サンプル数5  \n",
    "→ 情報利得0.143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d188e8b8-c721-4fe5-8a3f-23ae6e4193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ig(X,y):\n",
    "    # 親ノードの作成(X + y)\n",
    "    Xy = X + y\n",
    "    # 左右の子ノードの計算の前処理\n",
    "    n_l = np.sum(X)/np.sum(Xy) # 左ノード\n",
    "    n_r = np.sum(y)/np.sum(Xy) # 右ノード\n",
    "    # 情報利得を計算するための式\n",
    "    IG = gini(Xy) - (n_l*gini(X) + n_r*gini(y))\n",
    "\n",
    "    return IG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b867e-311f-461e-8da4-c0a6ae401036",
   "metadata": {},
   "source": [
    "## 【問題3】学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d291e-b84e-41f4-88cf-d0cb0bd27100",
   "metadata": {},
   "source": [
    "### 空間の分割を行い、決定木のグラフを生成するコードを作成してください。今は深さ1の決定木なので、分割を1回だけ行います。ここでグラフを生成するとは、1回の分割の際の条件としてどの特徴量がいくつ以上の時とするかを求めるということです。\n",
    "\n",
    "\n",
    "訓練データに対してすべての組み合わせの分割を行い、その中でノード間の情報利得が最大となる分割をそのノードの分割基準として記録します。\n",
    "\n",
    "\n",
    "クラスが混ざらない不純度が0のノード、または指定された深さのノードが 葉ノード となります。葉ノードにはクラスを記録しておき、これを推定時に分類するクラスとします。クラスが混ざらない場合はそのままのクラスを記録し、混ざっている場合は多数決により決めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea5efa-b973-4fcf-b2b1-3f4ac0b26bed",
   "metadata": {},
   "source": [
    "### 《組み合わせの取り方》"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668759c-d902-4113-94f2-5c2351ddb3ce",
   "metadata": {},
   "source": [
    "すべての組み合わせの取り方は、最も単純には各特徴量の値自体をしきい値にして分割を行う方法があります。片側の端は今回のスクラッチはこの方法で行なってください。\n",
    "\n",
    "\n",
    "他には中間の値をしきい値にする方法もあり、scikit-learnではこの方法が用いられています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c521ef-7c59-4350-90f5-dec1260a793a",
   "metadata": {},
   "source": [
    "### 《補足》"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4b3c2-662b-4c2d-a908-b5f8a226ba75",
   "metadata": {},
   "source": [
    "問題2の情報利得を計算する関数はこの問題3で利用する上では、親ノードの不純度 $I(p)$ は固定されるため、左右のノードの不純度の合計を計算するだけでも同じ結果が得られることになります。しかし、ここでは親ノードを考慮した情報利得を計算する実装を行なってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c12078-96be-40c1-8418-b80de44f1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X):\n",
    "    # 特徴量の閾値を探す\n",
    "    Ig_maX = 0\n",
    "    Ig_ind = 0\n",
    "    for i in range(X.shape[0]-1):    \n",
    "        print(Ig(X[i],X[i+1]))\n",
    "        if Ig_maX < Ig(X[i],X[i+1]):\n",
    "            Ig_maX = Ig(X[i],X[i+1])\n",
    "            Ig_ind = i\n",
    "            # 閾値（情報利得）が最大になるものを確定させる\n",
    "    \n",
    "    return Ig_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130212f-3dbd-46f3-8af0-ed4a1f0f97d6",
   "metadata": {},
   "source": [
    "## 【問題4】推定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817cde0-0dd1-4bad-969b-249938a3786a",
   "metadata": {},
   "source": [
    "### 推定する仕組みを実装してください。ScratchDecesionTreeClassifierDepth1クラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "### 入力されたデータの値を学習した条件で判定していき、どの葉ノードに到達するかを見ます。葉ノードにはクラスが記録されているので、これが推定値となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18d7fb0-83f7-414e-a886-ae88294e9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):         \n",
    "    # 閾値(ここでは一つ)を超えたら 1、それ以外は0へとそれぞれに分割\n",
    "    boX = np.array([])\n",
    "    for i in range(X.shape[0]):\n",
    "        Ig_data = X[i,0]\n",
    "        if Ig_data < X[fit(X),0]:\n",
    "            boX = np.append(boX,1)\n",
    "        else:\n",
    "            boX = np.append(boX,0)\n",
    "    return boX    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9e716-0351-4b06-94e9-c6f0a7843928",
   "metadata": {},
   "source": [
    "## 【問題5】学習と推定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bc516-aa47-4cc6-97ac-08b4f7989886",
   "metadata": {},
   "source": [
    "### 機械学習スクラッチ入門のSprintで用意したシンプルデータセット2の2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "### scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "### AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c8c738-2a18-4387-b144-f4c59ef420be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    深さ1の決定木分類器のスクラッチ実装\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        決定木分類器を学習する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        \"\"\"\n",
    "        # 特徴量の閾値を探す\n",
    "        Ig_maX = 0\n",
    "        Ig_ind = 0\n",
    "        for i in range(X.shape[0]-1):    \n",
    "            if Ig_maX < self.Ig(X[i],X[i+1]):\n",
    "                Ig_maX = self.Ig(X[i],X[i+1])\n",
    "                Ig_ind = i\n",
    "        # 閾値（情報利得）が最大になるものを確定させる\n",
    "        return Ig_ind\n",
    "    \n",
    "    \"\"\"ジニ不純度を計算する関数\"\"\"\n",
    "    def gini(self,X):\n",
    "        # サンプルの総数\n",
    "        sum = np.sum(X)\n",
    "        # シグマの繰り返しを保管する変数\n",
    "        sigma = 0\n",
    "        # サンプルの数だけ繰り返す\n",
    "        for i in range(X.shape[0]):\n",
    "            # ジニ不純度を計算する式\n",
    "            sigma += (X[i]/sum)**2\n",
    "        ans = 1 - sigma\n",
    "        # 求めた不純度を返す\n",
    "        return ans\n",
    "    \n",
    "    \"\"\"情報利得を計算する関数\"\"\"\n",
    "    def Ig(self,X,y):\n",
    "        # 親ノードの作成(X + y)\n",
    "        Xy = X + y\n",
    "        # 左右の子ノードの計算の前処理\n",
    "        n_l = np.sum(X)/np.sum(Xy) # 左ノード\n",
    "        n_r = np.sum(y)/np.sum(Xy) # 右ノード\n",
    "        # 情報利得を計算するための式\n",
    "        IG = gini(Xy) - (n_l*gini(X) + n_r*gini(y))\n",
    "\n",
    "        return IG\n",
    "    \n",
    "    \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        決定木分類器を使いラベルを推定する\n",
    "        \"\"\"\n",
    "        boX = np.array([])\n",
    "        for i in range(X.shape[0]):\n",
    "            Ig_data = X[i,0]\n",
    "            if Ig_data < X[self.fit(X),0]:\n",
    "                boX = np.append(boX,1)\n",
    "            else:\n",
    "                boX = np.append(boX,0)\n",
    "        return boX    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42608de-2b32-4662-967c-47dea456e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings                      #ワーニング関連のモジュール？\n",
    "warnings.filterwarnings('ignore')    #ワーニングが消える？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7da1972-218c-4df7-9c1b-4c0bc07aab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "    [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "    [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "    [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "    [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "    [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "    [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "    [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "    [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "    [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "    [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "    [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "    [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "    [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "    [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "    [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "    [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "    [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "    [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "    [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ],\n",
    "])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da54616-36e8-4284-b4ad-d0f86a94bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc3c5c2-3f2f-4901-9d8c-b0c722f7ff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 0 1 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "'''決定木モデルの作成'''\n",
    "\n",
    "#モジュール読み込み、モデル構築\n",
    "stdc = ScratchDecesionTreeClassifierDepth1()\n",
    "\n",
    "#モデルの学習\n",
    "\n",
    "stdc.fit(X_train)\n",
    "y_pred = stdc.predict(X_test)\n",
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b427b36c-36a3-4e29-a979-3b08b06872a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[2 4]\n",
      " [1 3]]\n",
      "accuracy =  0.5\n",
      "precision =  0.42857142857142855\n",
      "recall =  0.75\n",
      "f1 score =  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print('confusion matrix = \\n', confusion_matrix(y_test, y_pred))\n",
    "print('accuracy = ', accuracy_score(y_test, y_pred))\n",
    "print('precision = ', precision_score(y_test, y_pred))\n",
    "print('recall = ', recall_score(y_test, y_pred))\n",
    "print('f1 score = ', f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594a19f-eb53-4f75-9d71-72717e1e444e",
   "metadata": {},
   "source": [
    "## 【問題6】決定領域の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a34c5-78c0-41b7-aa67-747d9adb4058",
   "metadata": {},
   "source": [
    "### 決定領域を可視化してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268c0bb9-1906-4b88-a37b-464798947e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def decision_region(X, y, model, step=0.01, title='decision region', xlabel='xlabel', ylabel='ylabel', target_names=['versicolor', 'virginica']):\n",
    "    \"\"\"\n",
    "    2値分類を2次元の特徴量で学習したモデルの決定領域を描く。\n",
    "    背景の色が学習したモデルによる推定値から描画される。\n",
    "    散布図の点は訓練データまたは検証データである。\n",
    "    Parameters\n",
    "    ----------------\n",
    "    X : ndarray, shape(n_samples, 2)\n",
    "        特徴量\n",
    "    y : ndarray, shape(n_samples,)\n",
    "        ラベル\n",
    "    model : object\n",
    "        学習したモデルのインスンタスを入れる\n",
    "    step : float, (default : 0.1)\n",
    "        推定値を計算する間隔を設定する\n",
    "    title : str\n",
    "        グラフのタイトルの文章を与える\n",
    "    xlabel, ylabel : str\n",
    "        軸ラベルの文章を与える\n",
    "    target_names= : list of str\n",
    "        凡例の一覧を与える\n",
    "    \"\"\"\n",
    "    # setting\n",
    "    scatter_color = ['red', 'blue']\n",
    "    contourf_color = ['pink', 'skyblue']\n",
    "    n_class = 2\n",
    "    # pred\n",
    "    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, step), np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, step))\n",
    "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
    "    y_pred = model.predict(mesh).reshape(mesh_f0.shape)\n",
    "    # plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
    "    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n",
    "    for i, target in enumerate(set(y)):\n",
    "        plt.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')\n",
    "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "    plt.legend(handles=patches)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18da44-d3a2-40f7-9d0c-39a0dc0ee95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_region(X_test, y_test, model = stdc, step=0.01, title='STDC', xlabel='xlabel', ylabel='ylabel', target_names=['0','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4345c05-6e43-494f-a6d8-58b2d3e51963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
